[onega@localhost hadoop-2.7.3]$ JAVA_HOME=/usr/lib/jvm/java bin/hadoop
Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]
  CLASSNAME            run the class named CLASSNAME
 or
  where COMMAND is one of:
  fs                   run a generic filesystem user client
  version              print the version
  jar <jar>            run a jar file
                       note: please use "yarn jar" to launch
                             YARN applications, not this command.
  checknative [-a|-h]  check native hadoop and compression libraries availability
  distcp <srcurl> <desturl> copy file or directories recursively
  archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive
  classpath            prints the class path needed to get the
  credential           interact with credential providers
                       Hadoop jar and the required libraries
  daemonlog            get/set the log level for each daemon
  trace                view and modify Hadoop tracing settings

Most commands print help when invoked w/o parameters.

[onega@localhost hadoop-2.7.3]$ mkdir input
[onega@localhost hadoop-2.7.3]$ ls etc/hadoop/*xml
etc/hadoop/capacity-scheduler.xml  etc/hadoop/hadoop-policy.xml  etc/hadoop/httpfs-site.xml  etc/hadoop/kms-site.xml
etc/hadoop/core-site.xml           etc/hadoop/hdfs-site.xml      etc/hadoop/kms-acls.xml     etc/hadoop/yarn-site.xml
[onega@localhost hadoop-2.7.3]$ cp etc/hadoop/*xml input
[onega@localhost hadoop-2.7.3]$ JAVA_HOME=/usr/lib/jvm/java bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar grep input output 'dfs[a-z.]+'
17/01/28 12:50:17 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
17/01/28 12:50:17 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=
17/01/28 12:50:17 INFO input.FileInputFormat: Total input paths to process : 8
17/01/28 12:50:18 INFO mapreduce.JobSubmitter: number of splits:8
17/01/28 12:50:18 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local870292003_0001
17/01/28 12:50:18 INFO mapreduce.Job: The url to track the job: http://localhost:8080/
17/01/28 12:50:18 INFO mapreduce.Job: Running job: job_local870292003_0001
17/01/28 12:50:18 INFO mapred.LocalJobRunner: OutputCommitter set in config null
17/01/28 12:50:18 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/01/28 12:50:18 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17/01/28 12:50:18 INFO mapred.LocalJobRunner: Waiting for map tasks
17/01/28 12:50:18 INFO mapred.LocalJobRunner: Starting task: attempt_local870292003_0001_m_000000_0
17/01/28 12:50:18 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/01/28 12:50:18 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
17/01/28 12:50:19 INFO mapred.MapTask: Processing split: file:/home/onega/bin/hadoop-2.7.3/input/hadoop-policy.xml:0+9683
17/01/28 12:50:19 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
17/01/28 12:50:19 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
17/01/28 12:50:19 INFO mapred.MapTask: soft limit at 83886080
17/01/28 12:50:19 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
17/01/28 12:50:19 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
17/01/28 12:50:19 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
17/01/28 12:50:19 INFO mapred.LocalJobRunner:
17/01/28 12:50:19 INFO mapred.MapTask: Starting flush of map output
17/01/28 12:50:19 INFO mapred.MapTask: Spilling map output
17/01/28 12:50:19 INFO mapred.MapTask: bufstart = 0; bufend = 17; bufvoid = 104857600
17/01/28 12:50:19 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600
17/01/28 12:50:19 INFO mapred.MapTask: Finished spill 0
17/01/28 12:50:19 INFO mapred.Task: Task:attempt_local870292003_0001_m_000000_0 is done. And is in the process of committing
17/01/28 12:50:19 INFO mapred.LocalJobRunner: map
17/01/28 12:50:19 INFO mapred.Task: Task 'attempt_local870292003_0001_m_000000_0' done.
17/01/28 12:50:19 INFO mapred.LocalJobRunner: Finishing task: attempt_local870292003_0001_m_000000_0
17/01/28 12:50:19 INFO mapred.LocalJobRunner: Starting task: attempt_local870292003_0001_m_000001_0
17/01/28 12:50:19 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/01/28 12:50:19 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
17/01/28 12:50:19 INFO mapred.MapTask: Processing split: file:/home/onega/bin/hadoop-2.7.3/input/kms-site.xml:0+5511
17/01/28 12:50:19 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
17/01/28 12:50:19 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
17/01/28 12:50:19 INFO mapred.MapTask: soft limit at 83886080
17/01/28 12:50:19 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
17/01/28 12:50:19 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
17/01/28 12:50:19 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
17/01/28 12:50:19 INFO mapred.LocalJobRunner:
17/01/28 12:50:19 INFO mapred.MapTask: Starting flush of map output
17/01/28 12:50:19 INFO mapred.Task: Task:attempt_local870292003_0001_m_000001_0 is done. And is in the process of committing
17/01/28 12:50:19 INFO mapred.LocalJobRunner: map
17/01/28 12:50:19 INFO mapred.Task: Task 'attempt_local870292003_0001_m_000001_0' done.
17/01/28 12:50:19 INFO mapred.LocalJobRunner: Finishing task: attempt_local870292003_0001_m_000001_0
17/01/28 12:50:19 INFO mapred.LocalJobRunner: Starting task: attempt_local870292003_0001_m_000002_0
17/01/28 12:50:19 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/01/28 12:50:19 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
17/01/28 12:50:19 INFO mapred.MapTask: Processing split: file:/home/onega/bin/hadoop-2.7.3/input/capacity-scheduler.xml:0+4436
17/01/28 12:50:19 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
17/01/28 12:50:19 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
17/01/28 12:50:19 INFO mapred.MapTask: soft limit at 83886080
17/01/28 12:50:19 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
17/01/28 12:50:19 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
17/01/28 12:50:19 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
17/01/28 12:50:19 INFO mapred.LocalJobRunner:
17/01/28 12:50:19 INFO mapred.MapTask: Starting flush of map output
17/01/28 12:50:19 INFO mapred.Task: Task:attempt_local870292003_0001_m_000002_0 is done. And is in the process of committing
17/01/28 12:50:19 INFO mapred.LocalJobRunner: map
17/01/28 12:50:19 INFO mapred.Task: Task 'attempt_local870292003_0001_m_000002_0' done.
17/01/28 12:50:19 INFO mapred.LocalJobRunner: Finishing task: attempt_local870292003_0001_m_000002_0
17/01/28 12:50:19 INFO mapred.LocalJobRunner: Starting task: attempt_local870292003_0001_m_000003_0
17/01/28 12:50:19 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/01/28 12:50:19 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
17/01/28 12:50:19 INFO mapred.MapTask: Processing split: file:/home/onega/bin/hadoop-2.7.3/input/kms-acls.xml:0+3518
17/01/28 12:50:19 INFO mapreduce.Job: Job job_local870292003_0001 running in uber mode : false
17/01/28 12:50:19 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
17/01/28 12:50:19 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
17/01/28 12:50:19 INFO mapred.MapTask: soft limit at 83886080
17/01/28 12:50:19 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
17/01/28 12:50:19 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
17/01/28 12:50:19 INFO mapreduce.Job:  map 100% reduce 0%
17/01/28 12:50:19 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
17/01/28 12:50:19 INFO mapred.LocalJobRunner:
17/01/28 12:50:19 INFO mapred.MapTask: Starting flush of map output
17/01/28 12:50:19 INFO mapred.Task: Task:attempt_local870292003_0001_m_000003_0 is done. And is in the process of committing
17/01/28 12:50:19 INFO mapred.LocalJobRunner: map
17/01/28 12:50:19 INFO mapred.Task: Task 'attempt_local870292003_0001_m_000003_0' done.
17/01/28 12:50:19 INFO mapred.LocalJobRunner: Finishing task: attempt_local870292003_0001_m_000003_0
17/01/28 12:50:19 INFO mapred.LocalJobRunner: Starting task: attempt_local870292003_0001_m_000004_0
17/01/28 12:50:19 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/01/28 12:50:19 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
17/01/28 12:50:19 INFO mapred.MapTask: Processing split: file:/home/onega/bin/hadoop-2.7.3/input/hdfs-site.xml:0+775
17/01/28 12:50:19 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
17/01/28 12:50:19 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
17/01/28 12:50:19 INFO mapred.MapTask: soft limit at 83886080
17/01/28 12:50:19 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
17/01/28 12:50:19 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
17/01/28 12:50:19 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
17/01/28 12:50:19 INFO mapred.LocalJobRunner:
17/01/28 12:50:19 INFO mapred.MapTask: Starting flush of map output
17/01/28 12:50:19 INFO mapred.Task: Task:attempt_local870292003_0001_m_000004_0 is done. And is in the process of committing
17/01/28 12:50:19 INFO mapred.LocalJobRunner: map
17/01/28 12:50:19 INFO mapred.Task: Task 'attempt_local870292003_0001_m_000004_0' done.
17/01/28 12:50:19 INFO mapred.LocalJobRunner: Finishing task: attempt_local870292003_0001_m_000004_0
17/01/28 12:50:19 INFO mapred.LocalJobRunner: Starting task: attempt_local870292003_0001_m_000005_0
17/01/28 12:50:19 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/01/28 12:50:19 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
17/01/28 12:50:19 INFO mapred.MapTask: Processing split: file:/home/onega/bin/hadoop-2.7.3/input/core-site.xml:0+774
17/01/28 12:50:19 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
17/01/28 12:50:19 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
17/01/28 12:50:19 INFO mapred.MapTask: soft limit at 83886080
17/01/28 12:50:19 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
17/01/28 12:50:19 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
17/01/28 12:50:19 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
17/01/28 12:50:19 INFO mapred.LocalJobRunner:
17/01/28 12:50:19 INFO mapred.MapTask: Starting flush of map output
17/01/28 12:50:19 INFO mapred.Task: Task:attempt_local870292003_0001_m_000005_0 is done. And is in the process of committing
17/01/28 12:50:19 INFO mapred.LocalJobRunner: map
17/01/28 12:50:19 INFO mapred.Task: Task 'attempt_local870292003_0001_m_000005_0' done.
17/01/28 12:50:19 INFO mapred.LocalJobRunner: Finishing task: attempt_local870292003_0001_m_000005_0
17/01/28 12:50:19 INFO mapred.LocalJobRunner: Starting task: attempt_local870292003_0001_m_000006_0
17/01/28 12:50:19 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/01/28 12:50:19 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
17/01/28 12:50:19 INFO mapred.MapTask: Processing split: file:/home/onega/bin/hadoop-2.7.3/input/yarn-site.xml:0+690
17/01/28 12:50:19 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
17/01/28 12:50:19 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
17/01/28 12:50:19 INFO mapred.MapTask: soft limit at 83886080
17/01/28 12:50:19 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
17/01/28 12:50:19 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
17/01/28 12:50:19 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
17/01/28 12:50:19 INFO mapred.LocalJobRunner:
17/01/28 12:50:19 INFO mapred.MapTask: Starting flush of map output
17/01/28 12:50:19 INFO mapred.Task: Task:attempt_local870292003_0001_m_000006_0 is done. And is in the process of committing
17/01/28 12:50:19 INFO mapred.LocalJobRunner: map
17/01/28 12:50:19 INFO mapred.Task: Task 'attempt_local870292003_0001_m_000006_0' done.
17/01/28 12:50:19 INFO mapred.LocalJobRunner: Finishing task: attempt_local870292003_0001_m_000006_0
17/01/28 12:50:19 INFO mapred.LocalJobRunner: Starting task: attempt_local870292003_0001_m_000007_0
17/01/28 12:50:19 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/01/28 12:50:19 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
17/01/28 12:50:19 INFO mapred.MapTask: Processing split: file:/home/onega/bin/hadoop-2.7.3/input/httpfs-site.xml:0+620
17/01/28 12:50:20 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
17/01/28 12:50:20 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
17/01/28 12:50:20 INFO mapred.MapTask: soft limit at 83886080
17/01/28 12:50:20 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
17/01/28 12:50:20 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
17/01/28 12:50:20 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
17/01/28 12:50:20 INFO mapred.LocalJobRunner:
17/01/28 12:50:20 INFO mapred.MapTask: Starting flush of map output
17/01/28 12:50:20 INFO mapred.Task: Task:attempt_local870292003_0001_m_000007_0 is done. And is in the process of committing
17/01/28 12:50:20 INFO mapred.LocalJobRunner: map
17/01/28 12:50:20 INFO mapred.Task: Task 'attempt_local870292003_0001_m_000007_0' done.
17/01/28 12:50:20 INFO mapred.LocalJobRunner: Finishing task: attempt_local870292003_0001_m_000007_0
17/01/28 12:50:20 INFO mapred.LocalJobRunner: map task executor complete.
17/01/28 12:50:20 INFO mapred.LocalJobRunner: Waiting for reduce tasks
17/01/28 12:50:20 INFO mapred.LocalJobRunner: Starting task: attempt_local870292003_0001_r_000000_0
17/01/28 12:50:20 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/01/28 12:50:20 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
17/01/28 12:50:20 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3be8cbb1
17/01/28 12:50:20 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10
17/01/28 12:50:20 INFO reduce.EventFetcher: attempt_local870292003_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
17/01/28 12:50:20 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local870292003_0001_m_000003_0 decomp: 2 len: 6 to MEMORY
17/01/28 12:50:20 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local870292003_0001_m_000003_0
17/01/28 12:50:20 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2
17/01/28 12:50:20 WARN io.ReadaheadPool: Failed readahead on ifile
EBADF: Bad file descriptor
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)
	at org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/01/28 12:50:20 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local870292003_0001_m_000006_0 decomp: 2 len: 6 to MEMORY
17/01/28 12:50:20 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local870292003_0001_m_000006_0
17/01/28 12:50:20 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 2, commitMemory -> 2, usedMemory ->4
17/01/28 12:50:20 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local870292003_0001_m_000005_0 decomp: 2 len: 6 to MEMORY
17/01/28 12:50:20 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local870292003_0001_m_000005_0
17/01/28 12:50:20 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 3, commitMemory -> 4, usedMemory ->6
17/01/28 12:50:20 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local870292003_0001_m_000002_0 decomp: 2 len: 6 to MEMORY
17/01/28 12:50:20 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local870292003_0001_m_000002_0
17/01/28 12:50:20 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 4, commitMemory -> 6, usedMemory ->8
17/01/28 12:50:20 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local870292003_0001_m_000001_0 decomp: 2 len: 6 to MEMORY
17/01/28 12:50:20 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local870292003_0001_m_000001_0
17/01/28 12:50:20 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 5, commitMemory -> 8, usedMemory ->10
17/01/28 12:50:20 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local870292003_0001_m_000004_0 decomp: 2 len: 6 to MEMORY
17/01/28 12:50:20 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local870292003_0001_m_000004_0
17/01/28 12:50:20 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 6, commitMemory -> 10, usedMemory ->12
17/01/28 12:50:20 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local870292003_0001_m_000000_0 decomp: 21 len: 25 to MEMORY
17/01/28 12:50:20 INFO reduce.InMemoryMapOutput: Read 21 bytes from map-output for attempt_local870292003_0001_m_000000_0
17/01/28 12:50:20 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 21, inMemoryMapOutputs.size() -> 7, commitMemory -> 12, usedMemory ->33
17/01/28 12:50:20 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local870292003_0001_m_000007_0 decomp: 2 len: 6 to MEMORY
17/01/28 12:50:20 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local870292003_0001_m_000007_0
17/01/28 12:50:20 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 8, commitMemory -> 33, usedMemory ->35
17/01/28 12:50:20 WARN io.ReadaheadPool: Failed readahead on ifile
EBADF: Bad file descriptor
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)
	at org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/01/28 12:50:20 WARN io.ReadaheadPool: Failed readahead on ifile
EBADF: Bad file descriptor
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)
	at org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/01/28 12:50:20 WARN io.ReadaheadPool: Failed readahead on ifile
EBADF: Bad file descriptor
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)
	at org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/01/28 12:50:20 WARN io.ReadaheadPool: Failed readahead on ifile
EBADF: Bad file descriptor
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)
	at org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/01/28 12:50:20 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning
17/01/28 12:50:20 WARN io.ReadaheadPool: Failed readahead on ifile
EBADF: Bad file descriptor
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)
	at org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/01/28 12:50:20 INFO mapred.LocalJobRunner: 8 / 8 copied.
17/01/28 12:50:20 INFO reduce.MergeManagerImpl: finalMerge called with 8 in-memory map-outputs and 0 on-disk map-outputs
17/01/28 12:50:20 WARN io.ReadaheadPool: Failed readahead on ifile
EBADF: Bad file descriptor
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)
	at org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/01/28 12:50:20 WARN io.ReadaheadPool: Failed readahead on ifile
EBADF: Bad file descriptor
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)
	at org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/01/28 12:50:20 INFO mapred.Merger: Merging 8 sorted segments
17/01/28 12:50:20 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 10 bytes
17/01/28 12:50:20 INFO reduce.MergeManagerImpl: Merged 8 segments, 35 bytes to disk to satisfy reduce memory limit
17/01/28 12:50:20 INFO reduce.MergeManagerImpl: Merging 1 files, 25 bytes from disk
17/01/28 12:50:20 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
17/01/28 12:50:20 INFO mapred.Merger: Merging 1 sorted segments
17/01/28 12:50:20 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 10 bytes
17/01/28 12:50:20 INFO mapred.LocalJobRunner: 8 / 8 copied.
17/01/28 12:50:20 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
17/01/28 12:50:20 INFO mapred.Task: Task:attempt_local870292003_0001_r_000000_0 is done. And is in the process of committing
17/01/28 12:50:20 INFO mapred.LocalJobRunner: 8 / 8 copied.
17/01/28 12:50:20 INFO mapred.Task: Task attempt_local870292003_0001_r_000000_0 is allowed to commit now
17/01/28 12:50:20 INFO output.FileOutputCommitter: Saved output of task 'attempt_local870292003_0001_r_000000_0' to file:/home/onega/bin/hadoop-2.7.3/grep-temp-1982394080/_temporary/0/task_local870292003_0001_r_000000
17/01/28 12:50:20 INFO mapred.LocalJobRunner: reduce > reduce
17/01/28 12:50:20 INFO mapred.Task: Task 'attempt_local870292003_0001_r_000000_0' done.
17/01/28 12:50:20 INFO mapred.LocalJobRunner: Finishing task: attempt_local870292003_0001_r_000000_0
17/01/28 12:50:20 INFO mapred.LocalJobRunner: reduce task executor complete.
17/01/28 12:50:20 INFO mapreduce.Job:  map 100% reduce 100%
17/01/28 12:50:20 INFO mapreduce.Job: Job job_local870292003_0001 completed successfully
17/01/28 12:50:20 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=2896230
		FILE: Number of bytes written=5232791
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=745
		Map output records=1
		Map output bytes=17
		Map output materialized bytes=67
		Input split bytes=957
		Combine input records=1
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=67
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =8
		Failed Shuffles=0
		Merged Map outputs=8
		GC time elapsed (ms)=155
		Total committed heap usage (bytes)=2929721344
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters
		Bytes Read=26007
	File Output Format Counters
		Bytes Written=123
17/01/28 12:50:20 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
17/01/28 12:50:20 INFO input.FileInputFormat: Total input paths to process : 1
17/01/28 12:50:20 INFO mapreduce.JobSubmitter: number of splits:1
17/01/28 12:50:21 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1648623946_0002
17/01/28 12:50:21 INFO mapreduce.Job: The url to track the job: http://localhost:8080/
17/01/28 12:50:21 INFO mapreduce.Job: Running job: job_local1648623946_0002
17/01/28 12:50:21 INFO mapred.LocalJobRunner: OutputCommitter set in config null
17/01/28 12:50:21 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/01/28 12:50:21 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17/01/28 12:50:21 INFO mapred.LocalJobRunner: Waiting for map tasks
17/01/28 12:50:21 INFO mapred.LocalJobRunner: Starting task: attempt_local1648623946_0002_m_000000_0
17/01/28 12:50:21 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/01/28 12:50:21 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
17/01/28 12:50:21 INFO mapred.MapTask: Processing split: file:/home/onega/bin/hadoop-2.7.3/grep-temp-1982394080/part-r-00000:0+111
17/01/28 12:50:21 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
17/01/28 12:50:21 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
17/01/28 12:50:21 INFO mapred.MapTask: soft limit at 83886080
17/01/28 12:50:21 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
17/01/28 12:50:21 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
17/01/28 12:50:21 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
17/01/28 12:50:21 INFO mapred.LocalJobRunner:
17/01/28 12:50:21 INFO mapred.MapTask: Starting flush of map output
17/01/28 12:50:21 INFO mapred.MapTask: Spilling map output
17/01/28 12:50:21 INFO mapred.MapTask: bufstart = 0; bufend = 17; bufvoid = 104857600
17/01/28 12:50:21 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600
17/01/28 12:50:21 INFO mapred.MapTask: Finished spill 0
17/01/28 12:50:21 INFO mapred.Task: Task:attempt_local1648623946_0002_m_000000_0 is done. And is in the process of committing
17/01/28 12:50:21 INFO mapred.LocalJobRunner: map
17/01/28 12:50:21 INFO mapred.Task: Task 'attempt_local1648623946_0002_m_000000_0' done.
17/01/28 12:50:21 INFO mapred.LocalJobRunner: Finishing task: attempt_local1648623946_0002_m_000000_0
17/01/28 12:50:21 INFO mapred.LocalJobRunner: map task executor complete.
17/01/28 12:50:21 INFO mapred.LocalJobRunner: Waiting for reduce tasks
17/01/28 12:50:21 INFO mapred.LocalJobRunner: Starting task: attempt_local1648623946_0002_r_000000_0
17/01/28 12:50:21 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/01/28 12:50:21 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
17/01/28 12:50:21 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2d035365
17/01/28 12:50:21 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10
17/01/28 12:50:21 INFO reduce.EventFetcher: attempt_local1648623946_0002_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
17/01/28 12:50:21 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local1648623946_0002_m_000000_0 decomp: 21 len: 25 to MEMORY
17/01/28 12:50:21 INFO reduce.InMemoryMapOutput: Read 21 bytes from map-output for attempt_local1648623946_0002_m_000000_0
17/01/28 12:50:21 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 21, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->21
17/01/28 12:50:21 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning
17/01/28 12:50:21 INFO mapred.LocalJobRunner: 1 / 1 copied.
17/01/28 12:50:21 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
17/01/28 12:50:21 INFO mapred.Merger: Merging 1 sorted segments
17/01/28 12:50:21 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 11 bytes
17/01/28 12:50:21 INFO reduce.MergeManagerImpl: Merged 1 segments, 21 bytes to disk to satisfy reduce memory limit
17/01/28 12:50:21 INFO reduce.MergeManagerImpl: Merging 1 files, 25 bytes from disk
17/01/28 12:50:21 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
17/01/28 12:50:21 INFO mapred.Merger: Merging 1 sorted segments
17/01/28 12:50:21 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 11 bytes
17/01/28 12:50:21 INFO mapred.LocalJobRunner: 1 / 1 copied.
17/01/28 12:50:21 INFO mapred.Task: Task:attempt_local1648623946_0002_r_000000_0 is done. And is in the process of committing
17/01/28 12:50:21 INFO mapred.LocalJobRunner: 1 / 1 copied.
17/01/28 12:50:21 INFO mapred.Task: Task attempt_local1648623946_0002_r_000000_0 is allowed to commit now
17/01/28 12:50:21 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1648623946_0002_r_000000_0' to file:/home/onega/bin/hadoop-2.7.3/output/_temporary/0/task_local1648623946_0002_r_000000
17/01/28 12:50:21 INFO mapred.LocalJobRunner: reduce > reduce
17/01/28 12:50:21 INFO mapred.Task: Task 'attempt_local1648623946_0002_r_000000_0' done.
17/01/28 12:50:21 INFO mapred.LocalJobRunner: Finishing task: attempt_local1648623946_0002_r_000000_0
17/01/28 12:50:21 INFO mapred.LocalJobRunner: reduce task executor complete.
17/01/28 12:50:22 INFO mapreduce.Job: Job job_local1648623946_0002 running in uber mode : false
17/01/28 12:50:22 INFO mapreduce.Job:  map 100% reduce 100%
17/01/28 12:50:22 INFO mapreduce.Job: Job job_local1648623946_0002 completed successfully
17/01/28 12:50:22 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=1249574
		FILE: Number of bytes written=2324640
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Map output bytes=17
		Map output materialized bytes=25
		Input split bytes=132
		Combine input records=0
		Combine output records=0
		Reduce input groups=1
		Reduce shuffle bytes=25
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=514850816
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters
		Bytes Read=123
	File Output Format Counters
		Bytes Written=23
[onega@localhost hadoop-2.7.3]$

[onega@localhost hellohdfs]$ HADOOP_HDFS_HOME=/home/onega/bin/hadoop-2.7.3 make
g++ -o hellohdfs -I/home/onega/bin/hadoop-2.7.3/include hellohdfs.cpp -lhdfs -ljvm -L/home/onega/bin/hadoop-2.7.3/lib/native -L/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.60-2.b27.el7_1.x86_64/jre/lib/amd64/server

[onega@localhost hellohdfs]$ LD_LIBRARY_PATH=/home/onega/bin/hadoop-2.7.3/lib/native:/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.60-2.b27.el7_1.x86_64/jre/lib/amd64/server ./hellohdfs
Environment variable CLASSPATH not set!
getJNIEnv: getGlobalJNIEnv failed
Environment variable CLASSPATH not set!
getJNIEnv: getGlobalJNIEnv failed
Failed to open /tmp/testfile.txt for writing!

[onega@localhost hadoop-2.7.3]$ JAVA_HOME=/usr/lib/jvm/java bin/hadoop classpath
/home/onega/bin/hadoop-2.7.3/etc/hadoop:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/*:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/*:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/*:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/*:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/*:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/*:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/lib/*:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
[onega@localhost hadoop-2.7.3]$

CLASSPATH=$(JAVA_HOME=/usr/lib/jvm/java ~/bin/hadoop-2.7.3/bin/hadoop classpath) LD_LIBRARY_PATH=/home/onega/bin/hadoop-2.7.3/lib/native:/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.60-2.b27.el7_1.x86_64/jre/lib/amd64/server ./hellohdfs

[onega@localhost hellohdfs]$ CLASSPATH=$(JAVA_HOME=/usr/lib/jvm/java ~/bin/hadoop-2.7.3/bin/hadoop classpath) LD_LIBRARY_PATH=/home/onega/bin/hadoop-2.7.3/lib/native:/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.60-2.b27.el7_1.x86_64/jre/lib/amd64/server ./hellohdfs
loadFileSystems error:
(unable to get stack trace for java.lang.NoClassDefFoundError exception: ExceptionUtils::getStackTrace error.)
hdfsBuilderConnect(forceNewInstance=0, nn=default, port=0, kerbTicketCachePath=(NULL), userName=(NULL)) error:
(unable to get stack trace for java.lang.NoClassDefFoundError exception: ExceptionUtils::getStackTrace error.)
hdfsOpenFile(/tmp/testfile.txt): constructNewObjectOfPath error:
(unable to get stack trace for java.lang.NoClassDefFoundError exception: ExceptionUtils::getStackTrace error.)
Failed to open /tmp/testfile.txt for writing!
[onega@localhost hellohdfs]$

https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html

[onega@localhost hadoop-2.7.3]$ cp -p etc/hadoop/core-site.xml etc/hadoop/core-site.xml.orig
[onega@localhost hadoop-2.7.3]$ cp -p etc/hadoop/hdfs-site.xml etc/hadoop/hdfs-site.xml.orig
[onega@localhost hadoop-2.7.3]$ JAVA_HOME=/usr/lib/jvm/java bin/hdfs namenode -format
17/01/28 13:19:34 INFO namenode.NameNode: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = localhost/127.0.0.1
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/onega/bin/hadoop-2.7.3/etc/hadoop:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/onega/bin/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_60
************************************************************/
17/01/28 13:19:34 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
17/01/28 13:19:34 INFO namenode.NameNode: createNameNode [-format]
Formatting using clusterid: CID-9fa3481e-2f1f-4640-958b-641ab507a3fa
17/01/28 13:19:35 INFO namenode.FSNamesystem: No KeyProvider found.
17/01/28 13:19:35 INFO namenode.FSNamesystem: fsLock is fair:true
17/01/28 13:19:35 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
17/01/28 13:19:35 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
17/01/28 13:19:35 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
17/01/28 13:19:35 INFO blockmanagement.BlockManager: The block deletion will start around 2017 Jan 28 13:19:35
17/01/28 13:19:35 INFO util.GSet: Computing capacity for map BlocksMap
17/01/28 13:19:35 INFO util.GSet: VM type       = 64-bit
17/01/28 13:19:35 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB
17/01/28 13:19:35 INFO util.GSet: capacity      = 2^21 = 2097152 entries
17/01/28 13:19:35 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false
17/01/28 13:19:35 INFO blockmanagement.BlockManager: defaultReplication         = 1
17/01/28 13:19:35 INFO blockmanagement.BlockManager: maxReplication             = 512
17/01/28 13:19:35 INFO blockmanagement.BlockManager: minReplication             = 1
17/01/28 13:19:35 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
17/01/28 13:19:35 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000
17/01/28 13:19:35 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
17/01/28 13:19:35 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
17/01/28 13:19:35 INFO namenode.FSNamesystem: fsOwner             = onega (auth:SIMPLE)
17/01/28 13:19:35 INFO namenode.FSNamesystem: supergroup          = supergroup
17/01/28 13:19:35 INFO namenode.FSNamesystem: isPermissionEnabled = true
17/01/28 13:19:35 INFO namenode.FSNamesystem: HA Enabled: false
17/01/28 13:19:35 INFO namenode.FSNamesystem: Append Enabled: true
17/01/28 13:19:35 INFO util.GSet: Computing capacity for map INodeMap
17/01/28 13:19:35 INFO util.GSet: VM type       = 64-bit
17/01/28 13:19:35 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB
17/01/28 13:19:35 INFO util.GSet: capacity      = 2^20 = 1048576 entries
17/01/28 13:19:35 INFO namenode.FSDirectory: ACLs enabled? false
17/01/28 13:19:35 INFO namenode.FSDirectory: XAttrs enabled? true
17/01/28 13:19:35 INFO namenode.FSDirectory: Maximum size of an xattr: 16384
17/01/28 13:19:35 INFO namenode.NameNode: Caching file names occuring more than 10 times
17/01/28 13:19:35 INFO util.GSet: Computing capacity for map cachedBlocks
17/01/28 13:19:35 INFO util.GSet: VM type       = 64-bit
17/01/28 13:19:35 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB
17/01/28 13:19:35 INFO util.GSet: capacity      = 2^18 = 262144 entries
17/01/28 13:19:35 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
17/01/28 13:19:35 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
17/01/28 13:19:35 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
17/01/28 13:19:35 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
17/01/28 13:19:35 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
17/01/28 13:19:35 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
17/01/28 13:19:35 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
17/01/28 13:19:35 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
17/01/28 13:19:35 INFO util.GSet: Computing capacity for map NameNodeRetryCache
17/01/28 13:19:35 INFO util.GSet: VM type       = 64-bit
17/01/28 13:19:35 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
17/01/28 13:19:35 INFO util.GSet: capacity      = 2^15 = 32768 entries
17/01/28 13:19:35 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1854915370-127.0.0.1-1485638375795
17/01/28 13:19:35 INFO common.Storage: Storage directory /tmp/hadoop-onega/dfs/name has been successfully formatted.
17/01/28 13:19:35 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-onega/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression
17/01/28 13:19:36 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-onega/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 352 bytes saved in 0 seconds.
17/01/28 13:19:36 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
17/01/28 13:19:36 INFO util.ExitUtil: Exiting with status 0
17/01/28 13:19:36 INFO namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at localhost/127.0.0.1
************************************************************/
[onega@localhost hadoop-2.7.3]$

$ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
  $ cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
  $ chmod 0600 ~/.ssh/authorized_keys

  [onega@localhost hadoop-2.7.3]$ export JAVA_HOME=/usr/lib/jvm/java
  [onega@localhost hadoop-2.7.3]$ sbin/start-dfs.sh
  Starting namenodes on [localhost]
  localhost: Error: JAVA_HOME is not set and could not be found.
  localhost: Error: JAVA_HOME is not set and could not be found.
  Starting secondary namenodes [0.0.0.0]
  0.0.0.0: Error: JAVA_HOME is not set and could not be found.
  [onega@localhost hadoop-2.7.3]$


  [onega@localhost hadoop-2.7.3]$ grep JAVA_HOME ~/.bashrc
  export JAVA_HOME=/usr/lib/jvm/java

Restart Linux
[onega@localhost hadoop-2.7.3]$ printenv JAVA_HOME
/usr/lib/jvm/java
[onega@localhost hadoop-2.7.3]$ sbin/start-dfs.sh
Starting namenodes on [localhost]
localhost: starting namenode, logging to /home/onega/bin/hadoop-2.7.3/logs/hadoop-onega-namenode-localhost.localdomain.out
localhost: starting datanode, logging to /home/onega/bin/hadoop-2.7.3/logs/hadoop-onega-datanode-localhost.localdomain.out
Starting secondary namenodes [0.0.0.0]
0.0.0.0: starting secondarynamenode, logging to /home/onega/bin/hadoop-2.7.3/logs/hadoop-onega-secondarynamenode-localhost.localdomain.out
[onega@localhost hadoop-2.7.3]$

[onega@localhost hadoop-2.7.3]$ bin/hdfs dfs -rm -r /tmp
17/01/28 14:18:19 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted /tmp

[onega@localhost hellohdfs]$ CLASSPATH=$(JAVA_HOME=/usr/lib/jvm/java ~/bin/hadoop-2.7.3/bin/hadoop classpath --glob) LD_LIBRARY_PATH=/home/onega/bin/hadoop-2.7.3/lib/native:/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.60-2.b27.el7_1.x86_64/jre/lib/amd64/server:/home/onega/gcc-6.3.0/lib64 ./hellohdfs
hdfsOpenFile(/tmp/testfile.txt): FileSystem#append((Lorg/apache/hadoop/fs/Path;)Lorg/apache/hadoop/fs/FSDataOutputStream;) error:
java.io.FileNotFoundException: failed to append to non-existent file /tmp/testfile.txt for client 127.0.0.1
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFileInternal(FSNamesystem.java:2671)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFileInt(FSNamesystem.java:2982)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2950)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:655)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.append(ClientNamenodeProtocolServerSideTranslatorPB.java:421)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSClient.callAppend(DFSClient.java:1821)
	at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1877)
	at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1847)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:340)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:336)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:336)
	at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:318)
	at org.apache.hadoop.fs.FileSystem.append(FileSystem.java:1166)
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): failed to append to non-existent file /tmp/testfile.txt for client 127.0.0.1
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFileInternal(FSNamesystem.java:2671)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFileInt(FSNamesystem.java:2982)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2950)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:655)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.append(ClientNamenodeProtocolServerSideTranslatorPB.java:421)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)

	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.append(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.append(ClientNamenodeProtocolTranslatorPB.java:328)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.append(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.callAppend(DFSClient.java:1808)
	... 8 more
Failed to open /tmp/testfile.txt for writing!
[onega@localhost hellohdfs]$
